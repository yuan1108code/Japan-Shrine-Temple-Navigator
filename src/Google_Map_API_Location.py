#!/usr/bin/env python3
"""
Google Maps API è³‡æ–™æ’·å–ç¨‹å¼ - æœ€çµ‚æ•´åˆç‰ˆæœ¬
è‡ªå‹•ç‚ºç¦äº•ç¸£æ™¯é»æ’·å– Google Maps çš„è©³ç´°è³‡è¨Š
åŒ…å«å®Œæ•´çš„å®‰å…¨æ§åˆ¶ã€é‡è¤‡æª¢æŸ¥ã€é€²åº¦æ¢å¾©åŠŸèƒ½
"""

import json
import time
import requests
import os
import sys
from typing import Dict, List, Optional, Any, Set
from dataclasses import dataclass, asdict
from pathlib import Path
import logging
from datetime import datetime
import subprocess

# è¨­å®šæ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('google_maps_fetcher.log'),
        logging.StreamHandler()
    ]
)

@dataclass
class APIConfig:
    """API é…ç½®é¡åˆ¥"""
    api_key: str
    max_daily_calls: int = 400
    batch_size: int = 25
    request_delay: float = 1.2
    retry_count: int = 3
    timeout: int = 10

@dataclass
class APIUsageStats:
    """API ä½¿ç”¨é‡çµ±è¨ˆ"""
    find_place_calls: int = 0
    place_details_calls: int = 0
    successful_calls: int = 0
    failed_calls: int = 0
    skipped_duplicates: int = 0
    start_time: str = ""
    
    @property
    def total_calls(self) -> int:
        return self.find_place_calls + self.place_details_calls
    
    def save_stats(self, filename: str = "api_usage_stats.json") -> None:
        """ä¿å­˜ä½¿ç”¨é‡çµ±è¨ˆ"""
        stats_dict = asdict(self)
        stats_dict['end_time'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(stats_dict, f, ensure_ascii=False, indent=2)

@dataclass
class LocationInfo:
    """æ™¯é»åŸºæœ¬è³‡è¨Š"""
    city: str
    location: str
    latitude: float
    longitude: float
    
    def get_unique_key(self) -> str:
        """ç²å–æ™¯é»çš„å”¯ä¸€è­˜åˆ¥éµ"""
        return f"{self.city}_{self.location}_{self.latitude:.6f}_{self.longitude:.6f}"

@dataclass
class GooglePlaceDetails:
    """Google Places API å›å‚³çš„è©³ç´°è³‡è¨Š"""
    place_id: str
    name: str
    formatted_address: str
    phone_number: Optional[str] = None
    website: Optional[str] = None
    rating: Optional[float] = None
    user_ratings_total: Optional[int] = None
    price_level: Optional[int] = None
    opening_hours: Optional[Dict[str, Any]] = None
    photos: Optional[List[str]] = None
    reviews: Optional[List[Dict[str, Any]]] = None
    business_status: Optional[str] = None
    types: Optional[List[str]] = None

class SafetyChecker:
    """å®‰å…¨æª¢æŸ¥å™¨"""
    
    @staticmethod
    def check_api_key(api_key: str) -> bool:
        """æª¢æŸ¥ API é‡‘é‘°æ˜¯å¦æœ‰æ•ˆ"""
        if not api_key:
            return False
            
        # æ¸¬è©¦ä¸€å€‹ç°¡å–®çš„ Find Place è«‹æ±‚
        url = "https://maps.googleapis.com/maps/api/place/findplacefromtext/json"
        params = {
            'input': 'é¤Šæµ©é¤¨åº­åœ’ ç¦äº•å¸‚',
            'inputtype': 'textquery',
            'fields': 'place_id,name',
            'key': api_key
        }
        
        try:
            response = requests.get(url, params=params, timeout=10)
            response.raise_for_status()
            data = response.json()
            
            if data.get('status') == 'OK':
                logging.info("âœ… API é‡‘é‘°é©—è­‰æˆåŠŸ")
                return True
            elif data.get('status') == 'REQUEST_DENIED':
                logging.error("âŒ API é‡‘é‘°ç„¡æ•ˆæˆ–æœªå•Ÿç”¨ Places API")
                return False
            elif data.get('status') == 'OVER_QUERY_LIMIT':
                logging.error("âš ï¸ API æŸ¥è©¢é™åˆ¶å·²é”ä¸Šé™")
                return False
            else:
                logging.error(f"âš ï¸ API å›æ‡‰ç•°å¸¸: {data.get('status')}")
                return False
                
        except requests.RequestException as e:
            logging.error(f"âŒ API æ¸¬è©¦å¤±æ•—: {e}")
            return False
    
    @staticmethod
    def estimate_cost(location_count: int) -> bool:
        """ä¼°ç®—æˆæœ¬ä¸¦æª¢æŸ¥æ˜¯å¦å®‰å…¨"""
        estimated_calls = location_count * 2
        usage_rate = estimated_calls / 17000 * 100
        
        logging.info(f"ğŸ“Š æˆæœ¬ä¼°ç®—:")
        logging.info(f"æ™¯é»æ•¸é‡: {location_count}")
        logging.info(f"é ä¼° API èª¿ç”¨: {estimated_calls}")
        logging.info(f"å…è²»é¡åº¦ä½¿ç”¨ç‡: {usage_rate:.1f}%")
        
        if estimated_calls > 17000:
            logging.warning("âš ï¸ è­¦å‘Šï¼šå¯èƒ½è¶…éå…è²»é¡åº¦ï¼")
            return False
        elif estimated_calls > 10000:
            logging.warning("âš ï¸ æ³¨æ„ï¼šä½¿ç”¨ç‡è¼ƒé«˜ï¼Œå»ºè­°ç›£æ§")
        else:
            logging.info("âœ… åœ¨å®‰å…¨ç¯„åœå…§")
        
        return True

class DuplicateChecker:
    """é‡è¤‡æª¢æŸ¥å™¨"""
    
    def __init__(self):
        self.processed_keys: Set[str] = set()
        self.load_existing_data()
    
    def load_existing_data(self) -> None:
        """è¼‰å…¥å·²å­˜åœ¨çš„è³‡æ–™ä¾†å»ºç«‹é‡è¤‡æª¢æŸ¥æ¸…å–®"""
        # æª¢æŸ¥å„ç¨®å¯èƒ½çš„è¼¸å‡ºæª”æ¡ˆ
        output_files = [
            "../output/fukui_enhanced_locations.json",
            "../output/fukui_enhanced_locations_full.json"
        ]
        
        # æª¢æŸ¥é€²åº¦æª”æ¡ˆ
        progress_files = list(Path(".").glob("fukui_enhanced_progress_*.json"))
        
        all_files = output_files + [str(f) for f in progress_files]
        
        for file_path in all_files:
            if Path(file_path).exists():
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    
                    for item in data:
                        if 'original_data' in item:
                            original = item['original_data']
                            location = LocationInfo(
                                city=original['city'],
                                location=original['location'],
                                latitude=original['latitude'],
                                longitude=original['longitude']
                            )
                            self.processed_keys.add(location.get_unique_key())
                
                except Exception as e:
                    logging.warning(f"ç„¡æ³•è¼‰å…¥æª”æ¡ˆ {file_path}: {e}")
        
        if self.processed_keys:
            logging.info(f"ğŸ” ç™¼ç¾ {len(self.processed_keys)} å€‹å·²è™•ç†çš„æ™¯é»")
    
    def is_duplicate(self, location: LocationInfo) -> bool:
        """æª¢æŸ¥æ˜¯å¦ç‚ºé‡è¤‡æ™¯é»"""
        return location.get_unique_key() in self.processed_keys
    
    def mark_processed(self, location: LocationInfo) -> None:
        """æ¨™è¨˜æ™¯é»ç‚ºå·²è™•ç†"""
        self.processed_keys.add(location.get_unique_key())

class GoogleMapsAPIClient:
    """Google Maps API å®¢æˆ¶ç«¯ - å®Œæ•´åŠŸèƒ½ç‰ˆæœ¬"""
    
    def __init__(self, config: APIConfig):
        self.config = config
        self.places_base_url = "https://maps.googleapis.com/maps/api/place"
        self.session = requests.Session()
        self.usage_stats = APIUsageStats(start_time=datetime.now().strftime('%Y-%m-%d %H:%M:%S'))
        
    def check_daily_limit(self) -> bool:
        """æª¢æŸ¥æ˜¯å¦è¶…éæ¯æ—¥ä½¿ç”¨é™åˆ¶"""
        if self.usage_stats.total_calls >= self.config.max_daily_calls:
            logging.error(f"å·²é”åˆ°æ¯æ—¥APIèª¿ç”¨é™åˆ¶ ({self.config.max_daily_calls})")
            return False
        return True
        
    def find_place(self, query: str, location: tuple) -> Optional[str]:
        """ä½¿ç”¨ Find Place API æ‰¾åˆ°æœ€åŒ¹é…çš„åœ°é»"""
        if not self.check_daily_limit():
            return None
            
        url = f"{self.places_base_url}/findplacefromtext/json"
        
        params = {
            'input': query,
            'inputtype': 'textquery',
            'fields': 'place_id,name,geometry',
            'locationbias': f'circle:5000@{location[0]},{location[1]}',
            'language': 'ja',
            'key': self.config.api_key
        }
        
        for attempt in range(self.config.retry_count):
            try:
                self.usage_stats.find_place_calls += 1
                response = self.session.get(url, params=params, timeout=self.config.timeout)
                response.raise_for_status()
                data = response.json()
                
                if data.get('status') == 'OK' and data.get('candidates'):
                    self.usage_stats.successful_calls += 1
                    return data['candidates'][0]['place_id']
                elif data.get('status') == 'ZERO_RESULTS':
                    logging.warning(f"æ‰¾ä¸åˆ°åœ°é»: {query}")
                    return None
                elif data.get('status') == 'OVER_QUERY_LIMIT':
                    logging.error("API æŸ¥è©¢é™åˆ¶å·²é”ä¸Šé™")
                    return None
                else:
                    logging.warning(f"Find Place API å¤±æ•—: {data.get('status')}, é‡è©¦ {attempt + 1}/{self.config.retry_count}")
                    if attempt < self.config.retry_count - 1:
                        time.sleep(2 ** attempt)
                        
            except requests.RequestException as e:
                self.usage_stats.failed_calls += 1
                logging.error(f"Find Place API è«‹æ±‚å¤±æ•— (å˜—è©¦ {attempt + 1}): {e}")
                if attempt < self.config.retry_count - 1:
                    time.sleep(2 ** attempt)
                    
        return None
    
    def get_place_details(self, place_id: str) -> Optional[GooglePlaceDetails]:
        """ç²å–åœ°é»è©³ç´°è³‡è¨Š"""
        if not self.check_daily_limit():
            return None
            
        url = f"{self.places_base_url}/details/json"
        
        fields = [
            'place_id', 'name', 'formatted_address', 'formatted_phone_number',
            'website', 'rating', 'user_ratings_total', 'price_level',
            'opening_hours', 'photos', 'reviews', 'business_status', 'types'
        ]
        
        params = {
            'place_id': place_id,
            'fields': ','.join(fields),
            'language': 'ja',
            'key': self.config.api_key
        }
        
        for attempt in range(self.config.retry_count):
            try:
                self.usage_stats.place_details_calls += 1
                response = self.session.get(url, params=params, timeout=self.config.timeout)
                response.raise_for_status()
                data = response.json()
                
                if data.get('status') == 'OK':
                    result = data['result']
                    self.usage_stats.successful_calls += 1
                    
                    # è™•ç†ç‡Ÿæ¥­æ™‚é–“
                    opening_hours = None
                    if 'opening_hours' in result:
                        opening_hours = {
                            'open_now': result['opening_hours'].get('open_now'),
                            'weekday_text': result['opening_hours'].get('weekday_text', [])
                        }
                    
                    # è™•ç†ç…§ç‰‡ URL
                    photos = None
                    if 'photos' in result:
                        photos = []
                        for photo in result['photos'][:5]:
                            photo_url = f"{self.places_base_url}/photo?maxwidth=400&photoreference={photo['photo_reference']}&key={self.config.api_key}"
                            photos.append(photo_url)
                    
                    # è™•ç†è©•è«–
                    reviews = None
                    if 'reviews' in result:
                        reviews = []
                        for review in result['reviews'][:3]:
                            reviews.append({
                                'author_name': review.get('author_name'),
                                'rating': review.get('rating'),
                                'text': review.get('text'),
                                'time': review.get('time')
                            })
                    
                    return GooglePlaceDetails(
                        place_id=result.get('place_id', ''),
                        name=result.get('name', ''),
                        formatted_address=result.get('formatted_address', ''),
                        phone_number=result.get('formatted_phone_number'),
                        website=result.get('website'),
                        rating=result.get('rating'),
                        user_ratings_total=result.get('user_ratings_total'),
                        price_level=result.get('price_level'),
                        opening_hours=opening_hours,
                        photos=photos,
                        reviews=reviews,
                        business_status=result.get('business_status'),
                        types=result.get('types', [])
                    )
                elif data.get('status') == 'OVER_QUERY_LIMIT':
                    logging.error("API æŸ¥è©¢é™åˆ¶å·²é”ä¸Šé™")
                    return None
                else:
                    logging.warning(f"Place Details API å¤±æ•—: {data.get('status')}, é‡è©¦ {attempt + 1}/{self.config.retry_count}")
                    if attempt < self.config.retry_count - 1:
                        time.sleep(2 ** attempt)
                        
            except requests.RequestException as e:
                self.usage_stats.failed_calls += 1
                logging.error(f"Place Details API è«‹æ±‚å¤±æ•— (å˜—è©¦ {attempt + 1}): {e}")
                if attempt < self.config.retry_count - 1:
                    time.sleep(2 ** attempt)
                    
        return None

class FukuiLocationEnhancer:
    """ç¦äº•æ™¯é»è³‡æ–™å¢å¼·å™¨ - æœ€çµ‚å®Œæ•´ç‰ˆæœ¬"""
    
    def __init__(self, config: APIConfig):
        self.api_client = GoogleMapsAPIClient(config)
        self.duplicate_checker = DuplicateChecker()
        self.enhanced_data: List[Dict[str, Any]] = []
        self.config = config
        
    def load_fukui_locations(self, file_path: str) -> List[LocationInfo]:
        """è¼‰å…¥ç¦äº•æ™¯é»è³‡æ–™"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            locations = []
            for item in data:
                locations.append(LocationInfo(
                    city=item['city'],
                    location=item['location'],
                    latitude=item['coordinates']['latitude'],
                    longitude=item['coordinates']['longitude']
                ))
            
            logging.info(f"æˆåŠŸè¼‰å…¥ {len(locations)} å€‹æ™¯é»")
            return locations
            
        except Exception as e:
            logging.error(f"è¼‰å…¥æª”æ¡ˆå¤±æ•—: {e}")
            return []
    
    def enhance_location_data(self, locations: List[LocationInfo]) -> None:
        """å¢å¼·æ™¯é»è³‡æ–™ï¼ˆå«é‡è¤‡æª¢æŸ¥ï¼‰"""
        total = len(locations)
        processed_count = 0
        
        # éæ¿¾æ‰é‡è¤‡çš„æ™¯é»
        unique_locations = []
        for location in locations:
            if self.duplicate_checker.is_duplicate(location):
                self.api_client.usage_stats.skipped_duplicates += 1
                logging.info(f"è·³éé‡è¤‡æ™¯é»: {location.city} - {location.location}")
            else:
                unique_locations.append(location)
        
        if not unique_locations:
            logging.info("æ‰€æœ‰æ™¯é»éƒ½å·²è™•ç†éï¼Œç„¡éœ€é‡è¤‡è™•ç†")
            return
        
        remaining_calls = len(unique_locations) * 2
        logging.info(f"é è¨ˆè™•ç† {len(unique_locations)} å€‹æ–°æ™¯é»ï¼Œéœ€è¦ {remaining_calls} æ¬¡ API èª¿ç”¨")
        
        for i, location in enumerate(unique_locations, 1):
            logging.info(f"è™•ç†ä¸­ ({i}/{len(unique_locations)}): {location.city} - {location.location}")
            
            # å»ºç«‹æœå°‹æŸ¥è©¢
            search_query = f"{location.location} {location.city} ç¦äº•"
            location_coords = (location.latitude, location.longitude)
            
            # å°‹æ‰¾åœ°é»
            place_id = self.api_client.find_place(search_query, location_coords)
            
            enhanced_item = {
                'original_data': asdict(location),
                'google_maps_data': None,
                'search_query': search_query,
                'processed_at': time.strftime('%Y-%m-%d %H:%M:%S'),
                'processing_index': i - 1,
                'unique_key': location.get_unique_key()
            }
            
            if place_id:
                # ç²å–è©³ç´°è³‡è¨Š
                place_details = self.api_client.get_place_details(place_id)
                if place_details:
                    enhanced_item['google_maps_data'] = asdict(place_details)
                    logging.info(f"æˆåŠŸç²å–è³‡æ–™: {place_details.name}")
                else:
                    logging.warning(f"ç„¡æ³•ç²å–è©³ç´°è³‡æ–™: {location.location}")
            else:
                logging.warning(f"æ‰¾ä¸åˆ°å°æ‡‰çš„ Google Places: {location.location}")
            
            self.enhanced_data.append(enhanced_item)
            self.duplicate_checker.mark_processed(location)
            processed_count += 1
            
            # é¡¯ç¤ºä½¿ç”¨é‡çµ±è¨ˆ
            stats = self.api_client.usage_stats
            logging.info(f"API ä½¿ç”¨é‡ - ç¸½è¨ˆ: {stats.total_calls}, æˆåŠŸ: {stats.successful_calls}, å¤±æ•—: {stats.failed_calls}, è·³é: {stats.skipped_duplicates}")
            
            # API é™æµ
            time.sleep(self.config.request_delay)
            
            # å®šæœŸä¿å­˜é€²åº¦
            if i % self.config.batch_size == 0:
                self.save_progress(f"fukui_enhanced_progress_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
                self.api_client.usage_stats.save_stats(f"api_stats_{i}.json")
                
            # æª¢æŸ¥æ˜¯å¦é”åˆ°æ¯æ—¥é™åˆ¶
            if not self.api_client.check_daily_limit():
                logging.info(f"å·²é”åˆ°æ¯æ—¥é™åˆ¶ï¼Œåœæ­¢è™•ç†ã€‚å·²è™•ç† {i} å€‹æ™¯é»")
                break
    
    def save_progress(self, filename: str) -> None:
        """ä¿å­˜é€²åº¦"""
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(self.enhanced_data, f, ensure_ascii=False, indent=2)
            logging.info(f"é€²åº¦å·²ä¿å­˜åˆ°: {filename}")
        except Exception as e:
            logging.error(f"ä¿å­˜é€²åº¦å¤±æ•—: {e}")
    
    def save_final_results(self, output_path: str) -> None:
        """ä¿å­˜æœ€çµ‚çµæœ"""
        try:
            # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
            Path(output_path).parent.mkdir(exist_ok=True)
            
            # ä¿å­˜å®Œæ•´è³‡æ–™
            full_output_path = output_path.replace('.json', '_full.json')
            with open(full_output_path, 'w', encoding='utf-8') as f:
                json.dump(self.enhanced_data, f, ensure_ascii=False, indent=2)
            
            # ä¿å­˜ç°¡åŒ–ç‰ˆæœ¬ï¼ˆåªåŒ…å«æœ‰ Google è³‡æ–™çš„æ™¯é»ï¼‰
            successful_data = [
                item for item in self.enhanced_data 
                if item['google_maps_data'] is not None
            ]
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(successful_data, f, ensure_ascii=False, indent=2)
            
            # ä¿å­˜æœ€çµ‚ä½¿ç”¨é‡çµ±è¨ˆ
            self.api_client.usage_stats.save_stats("final_api_usage_stats.json")
            
            stats = self.api_client.usage_stats
            logging.info("="*50)
            logging.info("ğŸ“ˆ æœ€çµ‚çµ±è¨ˆå ±å‘Š:")
            logging.info(f"å®Œæ•´è³‡æ–™å·²ä¿å­˜åˆ°: {full_output_path}")
            logging.info(f"æˆåŠŸè³‡æ–™å·²ä¿å­˜åˆ°: {output_path}")
            logging.info(f"æ–°è™•ç†æ™¯é»: {len(self.enhanced_data)}")
            logging.info(f"æˆåŠŸç²å–: {len(successful_data)} å€‹æ™¯é»çš„ Google è³‡æ–™")
            logging.info(f"è·³éé‡è¤‡: {stats.skipped_duplicates}")
            logging.info(f"API ç¸½èª¿ç”¨: {stats.total_calls}")
            logging.info(f"æˆåŠŸç‡: {stats.successful_calls/max(stats.total_calls,1)*100:.1f}%")
            logging.info("="*50)
            
        except Exception as e:
            logging.error(f"ä¿å­˜æœ€çµ‚çµæœå¤±æ•—: {e}")

def load_config() -> APIConfig:
    """è¼‰å…¥é…ç½®è¨­å®š"""
    api_key = os.getenv("GOOGLE_MAPS_API_KEY", "")
    
    if not api_key:
        logging.error("\nâŒ æœªè¨­å®š Google Maps API é‡‘é‘°")
        logging.error("è«‹é¸æ“‡ä»¥ä¸‹æ–¹å¼ä¹‹ä¸€ï¼š")
        logging.error("1. è¨­å®šç’°å¢ƒè®Šæ•¸: export GOOGLE_MAPS_API_KEY=ä½ çš„APIé‡‘é‘°")
        logging.error("2. åŸ·è¡Œæ¸¬è©¦: python3 ../test_api.py")
        raise ValueError("å¿…é ˆæä¾› Google Maps API é‡‘é‘°")
    
    return APIConfig(
        api_key=api_key,
        max_daily_calls=int(os.getenv("MAX_DAILY_API_CALLS", "400")),
        batch_size=int(os.getenv("BATCH_SIZE", "25")),
        request_delay=float(os.getenv("REQUEST_DELAY", "1.2")),
        retry_count=int(os.getenv("RETRY_COUNT", "3")),
        timeout=int(os.getenv("TIMEOUT", "10"))
    )

def print_usage_info():
    """å°å‡ºä½¿ç”¨é‡è³‡è¨Š"""
    print("\n=== Google Maps API å…è²»é¡åº¦ ===")
    print("Find Place API: 17,000 æ¬¡/æœˆ")
    print("Place Details API: 17,000 æ¬¡/æœˆ")
    print("Places Photos API: 17,000 æ¬¡/æœˆ")
    print("\næ‚¨çš„æ™¯é»æ•¸é‡: 249")
    print("é ä¼° API èª¿ç”¨æ¬¡æ•¸: 498 (249 Ã— 2)")
    print("ä½”ç”¨å…è²»é¡åº¦æ¯”ä¾‹: 2.9%")
    print("=== å®‰å…¨ç¯„åœå…§ ===\n")

def pre_flight_checks(config: APIConfig, input_file: str) -> tuple[bool, int]:
    """åŸ·è¡Œèµ·é£›å‰æª¢æŸ¥"""
    logging.info("ğŸ›« åŸ·è¡Œèµ·é£›å‰å®‰å…¨æª¢æŸ¥...")
    
    # æª¢æŸ¥è³‡æ–™æª”æ¡ˆ
    if not Path(input_file).exists():
        logging.error(f"âŒ æ‰¾ä¸åˆ°è³‡æ–™æª”æ¡ˆ: {input_file}")
        return False, 0
    
    # è¼‰å…¥è³‡æ–™ä¸¦è¨ˆç®—æ•¸é‡
    try:
        with open(input_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        location_count = len(data)
        logging.info(f"âœ… è³‡æ–™æª”æ¡ˆæª¢æŸ¥é€šéï¼ŒåŒ…å« {location_count} å€‹æ™¯é»")
    except Exception as e:
        logging.error(f"âŒ è³‡æ–™æª”æ¡ˆè®€å–å¤±æ•—: {e}")
        return False, 0
    
    # æ¸¬è©¦ API é‡‘é‘°
    if not SafetyChecker.check_api_key(config.api_key):
        return False, 0
    
    # ä¼°ç®—æˆæœ¬
    if not SafetyChecker.estimate_cost(location_count):
        choice = input("\nâš ï¸ æ˜¯å¦ä»è¦ç¹¼çºŒåŸ·è¡Œï¼Ÿ(y/n): ").strip().lower()
        if choice != 'y':
            logging.info("ğŸ‘‹ ç”¨æˆ¶å–æ¶ˆåŸ·è¡Œ")
            return False, 0
    
    logging.info("âœ… æ‰€æœ‰å®‰å…¨æª¢æŸ¥é€šé")
    return True, location_count

def main():
    """ä¸»ç¨‹å¼"""
    logging.info("ğŸ—¾ ç¦äº•æ™¯é»è³‡æ–™å¢å¼·å·¥å…· - æœ€çµ‚æ•´åˆç‰ˆæœ¬")
    logging.info("="*60)
    
    try:
        # è¼‰å…¥é…ç½®
        config = load_config()
        masked_key = config.api_key[:10] + "..." + config.api_key[-4:] if len(config.api_key) > 14 else "***"
        logging.info(f"ğŸ”‘ ä½¿ç”¨ API é‡‘é‘°: {masked_key}")
        
    except ValueError as e:
        logging.error(f"é…ç½®éŒ¯èª¤: {e}")
        return
    
    # è¨­å®šæª”æ¡ˆè·¯å¾‘
    input_file = "../data/fukui_location.json"
    output_file = "../output/fukui_enhanced_locations.json"
    
    # é¡¯ç¤ºä½¿ç”¨é‡è³‡è¨Š
    print_usage_info()
    
    # åŸ·è¡Œèµ·é£›å‰æª¢æŸ¥
    checks_passed, location_count = pre_flight_checks(config, input_file)
    if not checks_passed:
        return
    
    # æœ€çµ‚ç¢ºèª
    logging.info("\n" + "="*60)
    choice = input("ğŸš€ æº–å‚™é–‹å§‹åŸ·è¡Œï¼Œæ˜¯å¦ç¹¼çºŒï¼Ÿ(y/n): ").strip().lower()
    
    if choice != 'y':
        logging.info("ğŸ‘‹ ç¨‹å¼çµæŸ")
        return
    
    # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
    Path("../output").mkdir(exist_ok=True)
    
    # å»ºç«‹å¢å¼·å™¨
    enhancer = FukuiLocationEnhancer(config)
    
    # è¼‰å…¥åŸå§‹è³‡æ–™
    locations = enhancer.load_fukui_locations(input_file)
    if not locations:
        logging.error("ç„¡æ³•è¼‰å…¥æ™¯é»è³‡æ–™ï¼Œç¨‹å¼çµæŸ")
        return
    
    # å¢å¼·è³‡æ–™
    logging.info("ğŸš€ é–‹å§‹å¢å¼·æ™¯é»è³‡æ–™...")
    try:
        enhancer.enhance_location_data(locations)
    except KeyboardInterrupt:
        logging.info("\nâ¸ï¸ ç¨‹å¼è¢«ç”¨æˆ¶ä¸­æ–·")
        logging.info("ğŸ“ é€²åº¦å·²ä¿å­˜ï¼Œå¯ç¨å¾Œç¹¼çºŒåŸ·è¡Œ")
    except Exception as e:
        logging.error(f"âŒ åŸ·è¡ŒéŒ¯èª¤: {e}")
    
    # ä¿å­˜çµæœ
    enhancer.save_final_results(output_file)
    
    logging.info("ğŸ‰ ç¨‹å¼åŸ·è¡Œå®Œæˆï¼")

if __name__ == "__main__":
    main() 